# Pipeline Cloud para An√°lise de Cota√ß√µes da B3 com Azure
 
## üìë √çndice
- [:pushpin: Introdu√ß√£o](https://github.com/thabus/Projeto_Cloud/blob/main/README.md#1-introdu%C3%A7%C3%A3o)
  - [‚ÑπÔ∏è Descri√ß√£o Geral do Sistema](https://github.com/thabus/Projeto_Cloud#11-descri%C3%A7%C3%A3o-geral-do-sistema)
  - [üéØ Objetivos do Projeto](https://github.com/thabus/Projeto_Cloud#12-objetivos-do-projeto)
- [:memo: Requisitos e Restri√ß√µes Arquiteturais](https://github.com/thabus/Projeto_Cloud/blob/main/README.md#3-requisitos-e-restri%C3%A7%C3%B5es-arquiterurais)
  - [üõ†Ô∏è Requisitos Funcionais](https://github.com/thabus/Projeto_Cloud#31-requisitos-funcionais)
  - [üìè Requisitos N√£o Funcionais](https://github.com/thabus/Projeto_Cloud#32-requisitos-n%C3%A3o-funcionais)
  - [‚õî Restri√ß√µes](https://github.com/thabus/Projeto_Cloud#33-restri%C3%A7%C3%B5es)
- [:open_file_folder: Casos de Uso](https://github.com/thabus/Projeto_Cloud?tab=readme-ov-file#4-casos-de-uso)
  - [üìå Casos de Uso do Sistema](https://github.com/thabus/Projeto_Cloud/blob/main/README.md#31-casos-de-uso-do-sistema)
  - [üìä Diagrama de Classes](https://github.com/thabus/Projeto_Cloud/blob/main/README.md#32-diagrama-de-classes)
  - [üîç Detalhamento dos Componentes e Relacionamentos](https://github.com/thabus/Projeto_Cloud/blob/main/README.md#33-detalhamento-dos-componentes-e-relacionamentos-do-diagrama)
- [:triangular_ruler: Vis√£o Geral da Arquitetura](https://github.com/thabus/Projeto_Cloud/blob/main/README.md#2-vis√£o-geral-da-arquitetura)
  - [üñºÔ∏è Descri√ß√£o da Arquitetura em Alto N√≠vel](https://github.com/thabus/Projeto_Cloud#21-descri%C3%A7%C3%A3o-da-arquitetura-em-alto-n%C3%ADvel)
  - [üíª Tecnologias e Padr√µes Utilizados](https://github.com/thabus/Projeto_Cloud#22-tecnologias-e-padr%C3%B5es-utilizados)

<br>


##  :pushpin: 1. Introdu√ß√£o

### 1.1. Descri√ß√£o Geral do Sistema
Esse √© um sistema em nuvem na plataforma Azure, projetado para a an√°lise de cota√ß√µes da Bolsa de Valores do Brasil (B3). O projeto aborda o desafio de processar os arquivos de cota√ß√µes di√°rias disponibilizados pela B3, que cont√™m informa√ß√µes como c√≥digo do ativo, data, pre√ßos de abertura, m√°ximo, m√≠nimo, fechamento e volume financeiro. A solu√ß√£o prop√µe um pipeline de dados completo para extrair, transformar, carregar (ETL) e analisar esses dados em larga escala, resultando na disponibiliza√ß√£o das informa√ß√µes para consumo em dashboards anal√≠ticos.

### 1.2. Objetivos do Projeto
O principal objetivo √© criar uma arquitetura de nuvem robusta e automatizada. Os objetivos espec√≠ficos incluem:
- Construir uma arquitetura para extra√ß√£o, transforma√ß√£o e carga de dados em grande escala.
- Aplicar de forma pr√°tica os conceitos de Big Data, ETL com Data Factory, Computa√ß√£o Serverless, Bancos de Dados em Nuvem e Containers Docker.
- Desenvolver e demonstrar habilidades em arquitetura de nuvem, integra√ß√£o de m√∫ltiplos servi√ßos e automa√ß√£o de pipelines de dados.

<br>

## :memo: 2. Requisitos e Restri√ß√µes Arquiterurais

### 2.1. Requisitos Funcionais
- **RF01: Ingest√£o de Dados:** O sistema deve ter a capacidade de simular a extra√ß√£o de arquivos di√°rios de cota√ß√µes da B3 e envi√°-los para o Azure Blob Storage.
- **RF02: Armazenamento de Arquivos:** O sistema deve armazenar os arquivos brutos (originais) e os arquivos j√° processados em um Azure Storage Account.
- **RF03: Transforma√ß√£o de Dados:** O sistema deve possuir um pipeline ETL para transformar os dados brutos em um formato estruturado e limpo, adequado para an√°lise posterior.
- **RF04: Extra√ß√£o de Informa√ß√µes:** O pipeline deve ser capaz de extrair as informa√ß√µes essenciais dos arquivos, como c√≥digo do ativo, data do preg√£o, pre√ßos de abertura, m√°ximo, m√≠nimo, fechamento e volume financeiro. A tabela final, no entanto, exige `Ativo`, `DataPregao`, `Abertura`, `Fechamento` e `Volume` .
- **RF05: Carga de Dados:** O sistema deve carregar os dados transformados em uma tabela espec√≠fica dentro de um Azure SQL Database.
- **RF06: Carga Incremental:** A carga de dados no banco de dados deve ser incremental, ou seja, deve adicionar apenas os novos dados a cada execu√ß√£o, sem duplicar registros existentes.
- **RF07: Automa√ß√£o de Alertas:** O sistema deve enviar notifica√ß√µes por e-mail para informar sobre o status da execu√ß√£o do pipeline (sucesso ou falha).
- **RF08: Disponibiliza√ß√£o para An√°lise:** Os dados finais armazenados no banco de dados devem estar dispon√≠veis para serem consumidos por ferramentas de visualiza√ß√£o, como Power BI ou Synapse Analytics.


### 2.2. Requisitos N√£o Funcionais
- **RNF01: Escalabilidade:** A arquitetura deve ser projetada para suportar o processamento e a an√°lise de dados em larga escala, conforme o objetivo educacional do projeto.
- **RNF02: Automa√ß√£o de Pipeline:** Todo o fluxo de dados, desde a ingest√£o at√© a carga final, deve ser automatizado para minimizar a necessidade de interven√ß√£o manual.
- **RNF03: Entreg√°veis Espec√≠ficos:** O projeto deve resultar em um conjunto de entreg√°veis bem definidos, incluindo um documento de arquitetura, o pipeline do Data Factory, o c√≥digo da Azure Function, o container Docker, a Logic App e os scripts SQL necess√°rios.


### 2.3. Restri√ß√µes
- **R01: Plataforma Cloud:** Toda a solu√ß√£o deve ser obrigatoriamente constru√≠da e hospedada na nuvem da Microsoft Azure. 
- **R02: Ferramenta de Orquestra√ß√£o:** O pipeline de ETL deve ser implementado exclusivamente com o Azure Data Factory. 
- **R03: Tecnologia de Carga de Dados:** A l√≥gica para a carga incremental de dados no banco de dados deve ser desenvolvida utilizando Azure Function (Serverless). 
- **R04: Linguagem da Fun√ß√£o:** A Azure Function para a carga de dados deve ser escrita na linguagem Python. 
- **R05: Banco de Dados:** O armazenamento dos dados processados e estruturados deve ser feito em um Azure SQL Database. 
- **R06: Ferramenta de Ingest√£o:** A simula√ß√£o da extra√ß√£o e envio de arquivos para a nuvem deve ser realizada por meio de um Container Docker. 
- **R07: Ferramenta de Automa√ß√£o de Alertas:** A automa√ß√£o de notifica√ß√µes e a integra√ß√£o com outros servi√ßos devem ser feitas com Logic Apps. 

<br>

## :open_file_folder: 3. Casos de Uso

### 3.1. Casos de Uso do Sistema

**Caso de Uso 1: Processar Pipeline de Dados de Cota√ß√µes**
- Nome do Caso de Uso: Processar Pipeline de Dados de Cota√ß√µes
- Ator Principal: Sistema (de forma automatizada)
- Ator Secund√°rio: Desenvolvedor/Operador
- Descri√ß√£o: Descreve o fluxo ponta-a-ponta, totalmente automatizado, para processar um arquivo de cota√ß√£o desde sua chegada na nuvem at√© a carga final no banco de dados e a notifica√ß√£o do resultado.
- Pr√©-condi√ß√µes:
   - A infraestrutura na Azure (Storage, Data Factory, Function, SQL DB, Logic Apps) est√° devidamente configurada.
   - Um novo arquivo de cota√ß√µes foi depositado na √°rea de dados brutos do Azure Storage Account.
- **Fluxo Principal (Caminho Feliz):**
   1. A chegada de um novo arquivo no Azure Storage aciona o pipeline no Azure Data Factory para iniciar o processamento.
   2. O pipeline l√™ o arquivo bruto, aplica as transforma√ß√µes para limpar e estruturar os dados e salva o resultado como um novo arquivo na √°rea de dados processados.
   3. Ao t√©rmino da etapa de transforma√ß√£o, o Data Factory aciona a Azure Function.
   4. A Azure Function l√™ o arquivo processado e realiza a carga incremental dos novos dados na tabela `Cotacoes` do Azure SQL Database.
   5. Ao final da execu√ß√£o bem-sucedida do pipeline, a Logic App √© acionada.
   6. A Logic App envia um e-mail de "Sucesso" para o Desenvolvedor/Operador.
 - **Fluxo de Exce√ß√£o:**
   1. Se ocorrer um erro em qualquer etapa do pipeline do Data Factory (transforma√ß√£o ou carga), a execu√ß√£o √© interrompida.
   2. A Logic App configurada para monitorar falhas √© acionada.
   3. A Logic App envia um e-mail de "Falha" para o Desenvolvedor/Operador, informando sobre o erro.
- P√≥s-condi√ß√µes:
   - Em caso de sucesso, os novos dados de cota√ß√µes est√£o dispon√≠veis para consulta no Azure SQL Database.
   - O Desenvolvedor/Operador recebe um e-mail informando o status final (sucesso ou falha) da execu√ß√£o do pipeline.


**Caso de Uso 2: Ingerir Arquivo de Cota√ß√µes**
- Nome do Caso de Uso: Ingerir Arquivo de Cota√ß√µes
- Ator Principal: Desenvolvedor
- Descri√ß√£o: Detalha o processo manual de simula√ß√£o da extra√ß√£o e envio de um arquivo de cota√ß√µes para a nuvem, que serve como gatilho para o "Caso de Uso 1".
- Pr√©-condi√ß√µes:
   - O container Docker est√° devidamente configurado e pronto para ser executado.
   - O Azure Storage Account est√° criado e acess√≠vel.
- **Fluxo Principal:**
    1.  O Desenvolvedor/Operador executa o container Docker.
    2.  O script dentro do container realiza o envio de um ou mais arquivos de cota√ß√µes para o container de dados brutos no Azure Blob Storage.
- **Fluxo de Exce√ß√£o:**
    1. Se o container Docker n√£o conseguir ser executado (ex: erro de configura√ß√£o do Docker na m√°quina local), o processo falha e uma mensagem de erro √© exibida no terminal do Desenvolvedor.
    2. Se o script dentro do container n√£o conseguir se conectar ao Azure Storage (ex: chave de acesso incorreta, falta de permiss√£o, problema de rede), o script termina com um erro e o arquivo n√£o √© enviado.
- P√≥s-condi√ß√µes:
   - Sucesso: O arquivo de cota√ß√µes est√° armazenado na √°rea de dados brutos da nuvem, pronto para acionar o pipeline de processamento.
   - Falha: O arquivo n√£o √© enviado para a nuvem e o pipeline principal n√£o √© acionado. Uma mensagem de erro √© registrada localmente.


**Caso de Uso 3: Analisar Dados de Cota√ß√µes**
- Nome do Caso de Uso: Analisar Dados de Cota√ß√µes
- Ator Principal: Analista de Dados
- Descri√ß√£o: Mostra como um usu√°rio final consome os dados j√° processados pelo pipeline para criar visualiza√ß√µes e gerar insights.
- Pr√©-condi√ß√µes:
    - O "Caso de Uso 1" foi executado com sucesso pelo menos uma vez.
    - Os dados de cota√ß√µes est√£o armazenados no Azure SQL Database.
    - O Analista de Dados possui uma ferramenta de visualiza√ß√£o (como Power BI) com as credenciais de acesso ao banco de dados.
- **Fluxo Principal:**
   1. O Analista de Dados utiliza uma ferramenta de visualiza√ß√£o, como Power BI ou Synapse Analytics.
   2. O Analista estabelece uma conex√£o com a base de dados Azure SQL Database que cont√©m as cota√ß√µes processadas.
   3. O Analista cria relat√≥rios e dashboards para analisar os dados.
- **Fluxo de Exce√ß√£o:**
    1. Se a ferramenta de visualiza√ß√£o n√£o conseguir se conectar ao Azure SQL Database (ex: credenciais inv√°lidas, firewall bloqueando a conex√£o, banco de dados offline), uma mensagem de erro de conex√£o √© exibida para o Analista na pr√≥pria ferramenta.
    2. Se uma consulta (query) executada pela ferramenta no banco de dados for inv√°lida, o banco retornar√° um erro que ser√° exibido na ferramenta de visualiza√ß√£o.
- P√≥s-condi√ß√µes:
    - Sucesso: As informa√ß√µes das cota√ß√µes s√£o apresentadas de forma visual, permitindo a an√°lise.
    - Falha: O Analista de Dados n√£o consegue acessar os dados e recebe uma mensagem de erro, precisando corrigir o problema de conex√£o ou da consulta para continuar. 



### 3.2. Diagrama de Classes

![Diagrama de Classes do Projeto](https://github.com/thabus/Projeto_Cloud/blob/main/diagrama_de_classes.png)

### 3.3 Detalhamento dos Componentes e Relacionamentos do Diagrama

- **Cotacao:** √â a classe que modela os dados de cota√ß√µes processados . Ela √© o destino final do nosso pipeline de dados.
- **DockerContainer:** Componente respons√°vel por iniciar o fluxo. Sua fun√ß√£o √© `enviarArquivoBruto()` para o Azure Storage.
- **AzureStorageAccount:** Atua como reposit√≥rio central (Data Lake). Armazena os arquivos brutos e os processados, al√©m de acionar a Azure Function quando um novo arquivo processado chega.
- **AzureDataFactory:** O orquestrador do ETL. Ele l√™ os dados brutos, executa a transforma√ß√£o e salva o resultado no Storage.
- **AzureFunction:** Componente serverless que cont√©m a l√≥gica de neg√≥cio para a carga. Ele √© acionado (`trigger`) pela chegada de um arquivo no Storage, l√™ esse arquivo e o carrega no banco de dados.
- **AzureSqlDatabase:** O banco de dados relacional que armazena permanentemente os objetos da classe `Cotacao`. A rela√ß√£o "1 para muitos" indica que o banco armazena m√∫ltiplas cota√ß√µes.
- **LogicApps:** Respons√°vel pela automa√ß√£o de alertas. Ele monitora o status do `AzureDataFactory` e envia notifica√ß√µes, desacoplando a l√≥gica de alerta do pipeline principal.

<br>


## :triangular_ruler: 4. Vis√£o Geral da Arquitetura

### 4.1. Descri√ß√£o da arquitetura em alto n√≠vel
A arquitetura do sistema √© desenhada como um pipeline de dados moderno e automatizado, implementado inteiramente na nuvem Microsoft Azure. O projeto segue o padr√£o de **ETL** para processar arquivos de cota√ß√µes da B3, desde sua ingest√£o at√© o armazenamento em um banco de dados relacional para an√°lise.
O fluxo arquitetural pode ser resumido em tr√™s fases principais:
 - 1. Extra√ß√£o (Extract): O processo √© iniciado pela ingest√£o de dados, simulada por um **Container Docker**. Este container envia os arquivos brutos de cota√ß√µes para um reposit√≥rio central no **Azure Storage Account**.
 - 2. Transforma√ß√£o (Transform): O **Azure Data Factory** atua como o orquestrador central do pipeline. Ele √© respons√°vel por ler os arquivos brutos do Storage, aplicar as transforma√ß√µes necess√°rias para limpar e estruturar os dados e, em seguida, salvar os arquivos processados de volta no Azure Storage Account.
 - 3. Carga (Load): Nesta fase, uma **Azure Function** √© acionada automaticamente. Utilizando um padr√£o de computa√ß√£o *serverless*, a fun√ß√£o realiza uma carga incremental dos dados processados diretamente no **Azure SQL Database**, que serve como o banco de dados anal√≠tico final.

Adicionalmente, o **Logic Apps** √© utilizado para automatizar notifica√ß√µes e alertas, como o envio de e-mails sobre o status da execu√ß√£o do pipeline, e para integrar o fluxo com servi√ßos de visualiza√ß√£o, como o Power BI.

### 4.2. Tecnologias e padr√µes utilizados

| **Tecnologia / Padr√£o**                 | **Fun√ß√£o no Projeto**                                                                                                        |
| --------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| **Padr√£o de Pipeline de Dados (ETL)**   | Estrutura o fluxo em tr√™s etapas (Extra√ß√£o, Transforma√ß√£o e Carga) orquestradas pelo Azure Data Factory.                     |
| **Computa√ß√£o Serverless**               | Implementada com Azure Function para a carga incremental no banco, permitindo execu√ß√£o sob demanda sem gerenciar servidores. |
| **Cont√™ineres (Docker)**                | Utilizados para simular a ingest√£o de arquivos, encapsulando depend√™ncias e garantindo portabilidade.                        |
| **Automa√ß√£o e Orquestra√ß√£o**            | Azure Data Factory gerencia o fluxo principal, enquanto Logic Apps automatiza notifica√ß√µes e integra servi√ßos.               |
| **Banco de Dados como Servi√ßo (DBaaS)** | Azure SQL Database armazena os dados processados, eliminando a necessidade de administrar servidores.                        |
| **Data Lake**                           | Azure Storage Account centraliza o armazenamento de arquivos brutos e processados.                                           |


<br>

<h4>Grupo: Tha√≠s Bustamante e Emilly Fernandes</h4>
