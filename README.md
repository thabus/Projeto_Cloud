# Pipeline Cloud para AnÃ¡lise de CotaÃ§Ãµes da B3 com Azure
Â 
## ğŸ“‘ Ãndice
- [:pushpin: IntroduÃ§Ã£o](#1-introduÃ§Ã£o)
  - [DescriÃ§Ã£o Geral do Sistema](#11-descriÃ§Ã£o-geral-do-sistema)
  - [Objetivos do Projeto](#12-objetivos-do-projeto)
- [:memo: Requisitos e RestriÃ§Ãµes Arquiteturais](#2-requisitos-e-restriÃ§Ãµes-arquiteturais)
  - [Requisitos Funcionais](#21-requisitos-funcionais)
  - [Requisitos NÃ£o Funcionais](#22-requisitos-nÃ£o-funcionais)
  - [RestriÃ§Ãµes](#23-restriÃ§Ãµes)
- [:open_file_folder: Casos de Uso](#3-casos-de-uso)
  - [Casos de Uso do Sistema](#31-casos-de-uso-do-sistema)
  - [Diagrama de Classes](#32-diagrama-de-classes)
  - [Detalhamento dos Componentes e Relacionamentos](#33-detalhamento-dos-componentes-e-relacionamentos-do-diagrama)
- [:triangular_ruler: VisÃ£o Geral da Arquitetura](#4-visÃ£o-geral-da-arquitetura)
  - [DescriÃ§Ã£o da Arquitetura em Alto NÃ­vel](#41-descriÃ§Ã£o-da-arquitetura-em-alto-nÃ­vel)
  - [Tecnologias e PadrÃµes Utilizados](#42-tecnologias-e-padrÃµes-utilizados)
- [:bar_chart: Diagrama de Arquitetura](#5-diagrama-de-arquitetura)
  - [Diagrama do Pipeline de Dados na Azure](#51-diagrama-do-pipeline-de-dados-na-azure)
  - [Detalhamento do Fluxo de Dados](#52-detalhamento-do-fluxo-de-dados)


<br>


##  :pushpin: 1. IntroduÃ§Ã£o

### 1.1. DescriÃ§Ã£o Geral do Sistema
Esse Ã© um sistema em nuvem na plataforma Azure, projetado para a anÃ¡lise de cotaÃ§Ãµes da Bolsa de Valores do Brasil (B3). O projeto aborda o desafio de processar os arquivos de cotaÃ§Ãµes diÃ¡rias disponibilizados pela B3, que contÃªm informaÃ§Ãµes como cÃ³digo do ativo, data, preÃ§os de abertura, mÃ¡ximo, mÃ­nimo, fechamento e volume financeiro. A soluÃ§Ã£o propÃµe um pipeline de dados completo para extrair, transformar, carregar (ETL) e analisar esses dados em larga escala, resultando na disponibilizaÃ§Ã£o das informaÃ§Ãµes para consumo em dashboards analÃ­ticos.

### 1.2. Objetivos do Projeto
O principal objetivo Ã© criar uma arquitetura de nuvem robusta e automatizada. Os objetivos especÃ­ficos incluem:
- Construir uma arquitetura para extraÃ§Ã£o, transformaÃ§Ã£o e carga de dados em grande escala.
- Aplicar de forma prÃ¡tica os conceitos de Big Data, ETL com Data Factory, ComputaÃ§Ã£o Serverless, Bancos de Dados em Nuvem e Containers Docker.
- Desenvolver e demonstrar habilidades em arquitetura de nuvem, integraÃ§Ã£o de mÃºltiplos serviÃ§os e automaÃ§Ã£o de pipelines de dados.

<br>

## :memo: 2. Requisitos e RestriÃ§Ãµes Arquiteturais

### 2.1. Requisitos Funcionais
- **RF01: IngestÃ£o de Dados:** O sistema deve ter a capacidade de simular a extraÃ§Ã£o de arquivos diÃ¡rios de cotaÃ§Ãµes da B3 e enviÃ¡-los para o Azure Blob Storage.
- **RF02: Armazenamento de Arquivos:** O sistema deve armazenar os arquivos brutos (originais) e os arquivos jÃ¡ processados em um Azure Storage Account.
- **RF03: TransformaÃ§Ã£o de Dados:** O sistema deve possuir um pipeline ETL para transformar os dados brutos em um formato estruturado e limpo, adequado para anÃ¡lise posterior.
- **RF04: ExtraÃ§Ã£o de InformaÃ§Ãµes:** O pipeline deve ser capaz de extrair as informaÃ§Ãµes essenciais dos arquivos, como cÃ³digo do ativo, data do pregÃ£o, preÃ§os de abertura, mÃ¡ximo, mÃ­nimo, fechamento e volume financeiro. A tabela final, no entanto, exige `Ativo`, `DataPregao`, `Abertura`, `Fechamento` e `Volume` .
- **RF05: Carga de Dados:** O sistema deve carregar os dados transformados em uma tabela especÃ­fica dentro de um Azure SQL Database.
- **RF06: Carga Incremental:** A carga de dados no banco de dados deve ser incremental, ou seja, deve adicionar apenas os novos dados a cada execuÃ§Ã£o, sem duplicar registros existentes.
- **RF07: AutomaÃ§Ã£o de Alertas:** O sistema deve enviar notificaÃ§Ãµes por e-mail para informar sobre o status da execuÃ§Ã£o do pipeline (sucesso ou falha).
- **RF08: DisponibilizaÃ§Ã£o para AnÃ¡lise:** Os dados finais armazenados no banco de dados devem estar disponÃ­veis para serem consumidos por ferramentas de visualizaÃ§Ã£o, como Power BI ou Synapse Analytics.


### 2.2. Requisitos NÃ£o Funcionais
- **RNF01: Escalabilidade:** A arquitetura deve ser projetada para suportar o processamento e a anÃ¡lise de dados em larga escala, conforme o objetivo educacional do projeto.
- **RNF02: AutomaÃ§Ã£o de Pipeline:** Todo o fluxo de dados, desde a ingestÃ£o atÃ© a carga final, deve ser automatizado para minimizar a necessidade de intervenÃ§Ã£o manual.
- **RNF03: EntregÃ¡veis EspecÃ­ficos:** O projeto deve resultar em um conjunto de entregÃ¡veis bem definidos, incluindo um documento de arquitetura, o pipeline do Data Factory, o cÃ³digo da Azure Function, o container Docker, a Logic App e os scripts SQL necessÃ¡rios.


### 2.3. RestriÃ§Ãµes
- **R01: Plataforma Cloud:** Toda a soluÃ§Ã£o deve ser obrigatoriamente construÃ­da e hospedada na nuvem da Microsoft Azure.Â 
- **R02: Ferramenta de OrquestraÃ§Ã£o:** O pipeline de ETL deve ser implementado exclusivamente com o Azure Data Factory.Â 
- **R03: Tecnologia de Carga de Dados:** A lÃ³gica para a carga incremental de dados no banco de dados deve ser desenvolvida utilizando Azure Function (Serverless).Â 
- **R04: Linguagem da FunÃ§Ã£o:** A Azure Function para a carga de dados deve ser escrita na linguagem Python.Â 
- **R05: Banco de Dados:** O armazenamento dos dados processados e estruturados deve ser feito em um Azure SQL Database.Â 
- **R06: Ferramenta de IngestÃ£o:** A simulaÃ§Ã£o da extraÃ§Ã£o e envio de arquivos para a nuvem deve ser realizada por meio de um Container Docker.Â 
- **R07: Ferramenta de AutomaÃ§Ã£o de Alertas:** A automaÃ§Ã£o de notificaÃ§Ãµes e a integraÃ§Ã£o com outros serviÃ§os devem ser feitas com Logic Apps.Â 

<br>

## :open_file_folder: 3. Casos de Uso

### 3.1. Casos de Uso do Sistema

**Caso de Uso 1: Processar Pipeline de Dados de CotaÃ§Ãµes**
- Nome do Caso de Uso: Processar Pipeline de Dados de CotaÃ§Ãµes
- Ator Principal: Sistema (de forma automatizada)
- Ator SecundÃ¡rio: Desenvolvedor/Operador
- DescriÃ§Ã£o: Descreve o fluxo ponta-a-ponta, totalmente automatizado, para processar um arquivo de cotaÃ§Ã£o desde sua chegada na nuvem atÃ© a carga final no banco de dados e a notificaÃ§Ã£o do resultado.
- PrÃ©-condiÃ§Ãµes:
Â  Â - A infraestrutura na Azure (Storage, Data Factory, Function, SQL DB, Logic Apps) estÃ¡ devidamente configurada.
Â  Â - Um novo arquivo de cotaÃ§Ãµes foi depositado na Ã¡rea de dados brutos do Azure Storage Account.
- **Fluxo Principal (Caminho Feliz):**
Â  Â 1. A chegada de um novo arquivo no Azure Storage aciona o pipeline no Azure Data Factory para iniciar o processamento.
Â  Â 2. O pipeline lÃª o arquivo bruto, aplica as transformaÃ§Ãµes para limpar e estruturar os dados e salva o resultado como um novo arquivo na Ã¡rea de dados processados.
Â  Â 3. Ao tÃ©rmino da etapa de transformaÃ§Ã£o, o Data Factory aciona a Azure Function.
Â  Â 4. A Azure Function lÃª o arquivo processado e realiza a carga incremental dos novos dados na tabela `Cotacoes` do Azure SQL Database.
Â  Â 5. Ao final da execuÃ§Ã£o bem-sucedida do pipeline, a Logic App Ã© acionada.
Â  Â 6. A Logic App envia um e-mail de "Sucesso" para o Desenvolvedor/Operador.
Â - **Fluxo de ExceÃ§Ã£o:**
Â  Â 1. Se ocorrer um erro em qualquer etapa do pipeline do Data Factory (transformaÃ§Ã£o ou carga), a execuÃ§Ã£o Ã© interrompida.
Â  Â 2. A Logic App configurada para monitorar falhas Ã© acionada.
Â  Â 3. A Logic App envia um e-mail de "Falha" para o Desenvolvedor/Operador, informando sobre o erro.
- PÃ³s-condiÃ§Ãµes:
Â  Â - Em caso de sucesso, os novos dados de cotaÃ§Ãµes estÃ£o disponÃ­veis para consulta no Azure SQL Database.
Â  Â - O Desenvolvedor/Operador recebe um e-mail informando o status final (sucesso ou falha) da execuÃ§Ã£o do pipeline.


**Caso de Uso 2: Ingerir Arquivo de CotaÃ§Ãµes**
- Nome do Caso de Uso: Ingerir Arquivo de CotaÃ§Ãµes
- Ator Principal: Desenvolvedor
- DescriÃ§Ã£o: Detalha o processo manual de simulaÃ§Ã£o da extraÃ§Ã£o e envio de um arquivo de cotaÃ§Ãµes para a nuvem, que serve como gatilho para o "Caso de Uso 1".
- PrÃ©-condiÃ§Ãµes:
Â  Â - O container Docker estÃ¡ devidamente configurado e pronto para ser executado.
Â  Â - O Azure Storage Account estÃ¡ criado e acessÃ­vel.
- **Fluxo Principal:**
Â  Â  1.Â  O Desenvolvedor/Operador executa o container Docker.
Â  Â  2.Â  O script dentro do container realiza o envio de um ou mais arquivos de cotaÃ§Ãµes para o container de dados brutos no Azure Blob Storage.
- **Fluxo de ExceÃ§Ã£o:**
Â  Â  1. Se o container Docker nÃ£o conseguir ser executado (ex: erro de configuraÃ§Ã£o do Docker na mÃ¡quina local), o processo falha e uma mensagem de erro Ã© exibida no terminal do Desenvolvedor.
Â  Â  2. Se o script dentro do container nÃ£o conseguir se conectar ao Azure Storage (ex: chave de acesso incorreta, falta de permissÃ£o, problema de rede), o script termina com um erro e o arquivo nÃ£o Ã© enviado.
- PÃ³s-condiÃ§Ãµes:
Â  Â - Sucesso: O arquivo de cotaÃ§Ãµes estÃ¡ armazenado na Ã¡rea de dados brutos da nuvem, pronto para acionar o pipeline de processamento.
Â  Â - Falha: O arquivo nÃ£o Ã© enviado para a nuvem e o pipeline principal nÃ£o Ã© acionado. Uma mensagem de erro Ã© registrada localmente.


**Caso de Uso 3: Analisar Dados de CotaÃ§Ãµes**
- Nome do Caso de Uso: Analisar Dados de CotaÃ§Ãµes
- Ator Principal: Analista de Dados
- DescriÃ§Ã£o: Mostra como um usuÃ¡rio final consome os dados jÃ¡ processados pelo pipeline para criar visualizaÃ§Ãµes e gerar insights.
- PrÃ©-condiÃ§Ãµes:
Â  Â  - O "Caso de Uso 1" foi executado com sucesso pelo menos uma vez.
Â  Â  - Os dados de cotaÃ§Ãµes estÃ£o armazenados no Azure SQL Database.
Â  Â  - O Analista de Dados possui uma ferramenta de visualizaÃ§Ã£o (como Power BI) com as credenciais de acesso ao banco de dados.
- **Fluxo Principal:**
Â  Â 1. O Analista de Dados utiliza uma ferramenta de visualizaÃ§Ã£o, como Power BI ou Synapse Analytics.
Â  Â 2. O Analista estabelece uma conexÃ£o com a base de dados Azure SQL Database que contÃ©m as cotaÃ§Ãµes processadas.
Â  Â 3. O Analista cria relatÃ³rios e dashboards para analisar os dados.
- **Fluxo de ExceÃ§Ã£o:**
Â  Â  1. Se a ferramenta de visualizaÃ§Ã£o nÃ£o conseguir se conectar ao Azure SQL Database (ex: credenciais invÃ¡lidas, firewall bloqueando a conexÃ£o, banco de dados offline), uma mensagem de erro de conexÃ£o Ã© exibida para o Analista na prÃ³pria ferramenta.
Â  Â  2. Se uma consulta (query) executada pela ferramenta no banco de dados for invÃ¡lida, o banco retornarÃ¡ um erro que serÃ¡ exibido na ferramenta de visualizaÃ§Ã£o.
- PÃ³s-condiÃ§Ãµes:
Â  Â  - Sucesso: As informaÃ§Ãµes das cotaÃ§Ãµes sÃ£o apresentadas de forma visual, permitindo a anÃ¡lise.
Â  Â  - Falha: O Analista de Dados nÃ£o consegue acessar os dados e recebe uma mensagem de erro, precisando corrigir o problema de conexÃ£o ou da consulta para continuar.Â 



### 3.2. Diagrama de Classes

![Diagrama de Classes do Projeto](https://github.com/thabus/Projeto_Cloud/blob/main/imagens/diagrama_de_classes.png)

### 3.3 Detalhamento dos Componentes e Relacionamentos do Diagrama

- **Cotacao:** Ã‰ a classe que modela os dados de cotaÃ§Ãµes processados . Ela Ã© o destino final do nosso pipeline de dados.
- **DockerContainer:** Componente responsÃ¡vel por iniciar o fluxo. Sua funÃ§Ã£o Ã© `enviarArquivoBruto()` para o Azure Storage.
- **AzureStorageAccount:** Atua como repositÃ³rio central (Data Lake). Armazena os arquivos brutos e os processados, alÃ©m de acionar a Azure Function quando um novo arquivo processado chega.
- **AzureDataFactory:** O orquestrador do ETL. Ele lÃª os dados brutos, executa a transformaÃ§Ã£o e salva o resultado no Storage.
- **AzureFunction:** Componente serverless que contÃ©m a lÃ³gica de negÃ³cio para a carga. Ele Ã© acionado (`trigger`) pela chegada de um arquivo no Storage, lÃª esse arquivo e o carrega no banco de dados.
- **AzureSqlDatabase:** O banco de dados relacional que armazena permanentemente os objetos da classe `Cotacao`. A relaÃ§Ã£o "1 para muitos" indica que o banco armazena mÃºltiplas cotaÃ§Ãµes.
- **LogicApps:** ResponsÃ¡vel pela automaÃ§Ã£o de alertas. Ele monitora o status do `AzureDataFactory` e envia notificaÃ§Ãµes, desacoplando a lÃ³gica de alerta do pipeline principal.

<br>


## :triangular_ruler: 4. VisÃ£o Geral da Arquitetura

### 4.1. DescriÃ§Ã£o da arquitetura em alto nÃ­vel
A arquitetura do sistema Ã© desenhada como um pipeline de dados moderno e automatizado, implementado inteiramente na nuvem Microsoft Azure. O projeto segue o padrÃ£o de **ETL** para processar arquivos de cotaÃ§Ãµes da B3, desde sua ingestÃ£o atÃ© o armazenamento em um banco de dados relacional para anÃ¡lise.
O fluxo arquitetural pode ser resumido em trÃªs fases principais:
Â - 1. ExtraÃ§Ã£o (Extract): O processo Ã© iniciado pela ingestÃ£o de dados, simulada por um **Container Docker**. Este container envia os arquivos brutos de cotaÃ§Ãµes para um repositÃ³rio central no **Azure Storage Account**.
Â - 2. TransformaÃ§Ã£o (Transform): O **Azure Data Factory** atua como o orquestrador central do pipeline. Ele Ã© responsÃ¡vel por ler os arquivos brutos do Storage, aplicar as transformaÃ§Ãµes necessÃ¡rias para limpar e estruturar os dados e, em seguida, salvar os arquivos processados de volta no Azure Storage Account.
Â - 3. Carga (Load): Nesta fase, uma **Azure Function** Ã© acionada automaticamente. Utilizando um padrÃ£o de computaÃ§Ã£o *serverless*, a funÃ§Ã£o realiza uma carga incremental dos dados processados diretamente no **Azure SQL Database**, que serve como o banco de dados analÃ­tico final.

Adicionalmente, o **Logic Apps** Ã© utilizado para automatizar notificaÃ§Ãµes e alertas, como o envio de e-mails sobre o status da execuÃ§Ã£o do pipeline, e para integrar o fluxo com serviÃ§os de visualizaÃ§Ã£o, como o Power BI.

### 4.2. Tecnologias e padrÃµes utilizados

| **Tecnologia / PadrÃ£o**Â  Â  Â  Â  Â  Â  Â  Â  Â | **FunÃ§Ã£o no Projeto**Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  |
| --------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| **PadrÃ£o de Pipeline de Dados (ETL)**Â  Â | Estrutura o fluxo em trÃªs etapas (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga) orquestradas pelo Azure Data Factory.Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |
| **ComputaÃ§Ã£o Serverless**Â  Â  Â  Â  Â  Â  Â  Â | Implementada com Azure Function para a carga incremental no banco, permitindo execuÃ§Ã£o sob demanda sem gerenciar servidores. |
| **ContÃªineres (Docker)**Â  Â  Â  Â  Â  Â  Â  Â  | Utilizados para simular a ingestÃ£o de arquivos, encapsulando dependÃªncias e garantindo portabilidade.Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  |
| **AutomaÃ§Ã£o e OrquestraÃ§Ã£o**Â  Â  Â  Â  Â  Â  | Azure Data Factory gerencia o fluxo principal, enquanto Logic Apps automatiza notificaÃ§Ãµes e integra serviÃ§os.Â  Â  Â  Â  Â  Â  Â  Â |
| **Banco de Dados como ServiÃ§o (DBaaS)** | Azure SQL Database armazena os dados processados, eliminando a necessidade de administrar servidores.Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  |
| **Data Lake**Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â | Azure Storage Account centraliza o armazenamento de arquivos brutos e processados.Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |

<br>

<br>

## :bar_chart: Diagrama de Arquitetura

### 5.1. Diagrama do Pipeline de Dados na Azure
Este diagrama visualiza a arquitetura do pipeline de dados, detalhando o fluxo de informaÃ§Ãµes entre os principais serviÃ§os da Microsoft Azure e os componentes do projeto. Ele representa a jornada dos dados, desde a ingestÃ£o simulada atÃ© a carga final para anÃ¡lise.

![Diagrama de Arquitetura do Pipeline de Dados para AnÃ¡lise de CotaÃ§Ãµes da B3 com Azure](https://github.com/thabus/Projeto_Cloud/blob/main/imagens/Arquitetura_do_Pipeline_B3_no_Azure.png?raw=true)

### 5.2. Detalhamento do Fluxo de Dados
A seguir, detalhamos cada etapa do fluxo apresentado no diagrama:

- **1. IngestÃ£o (Container Docker):**
  - A simulaÃ§Ã£o da extraÃ§Ã£o dos arquivos de cotaÃ§Ãµes da B3 Ã© realizada por um **Container Docker**. Este contÃªiner envia os arquivos brutos para um contÃªiner especÃ­fico no Azure Blob Storage, conhecido como `raw zone`.

- **2. OrquestraÃ§Ã£o e TransformaÃ§Ã£o (Azure Data Factory):**
  - A chegada de um novo arquivo na `raw zone` atua como um gatilho para o **Azure Data Factory**. Ele orquestra o pipeline de ETL, lendo o arquivo bruto e aplicando as transformaÃ§Ãµes necessÃ¡rias para limpar e estruturar os dados. O resultado desse processamento Ã© salvo em um segundo contÃªiner no mesmo Storage Account, a `processed zone`.

- **3. Carga Incremental (Azure Function):**
  - Um segundo gatilho, desta vez na `processed zone`, aciona uma **Azure Function**. Esta funÃ§Ã£o, desenvolvida em Python, Ã© responsÃ¡vel por ler o arquivo processado e realizar a carga incremental dos dados na tabela `Cotacoes` do **Azure SQL Database**.

- **4. Monitoramento e NotificaÃ§Ãµes (Logic Apps):**
  - Paralelamente ao pipeline principal, o **Logic Apps** monitora o status da execuÃ§Ã£o do Azure Data Factory. Em caso de sucesso ou falha, ele envia notificaÃ§Ãµes automÃ¡ticas por e-mail, garantindo a visibilidade e o controle sobre o processo.

- **5. AnÃ¡lise (Azure SQL Database):**
  - Os dados, agora limpos e organizados no **Azure SQL Database**, estÃ£o prontos para serem consumidos por ferramentas de visualizaÃ§Ã£o, como o Power BI, permitindo a criaÃ§Ã£o de dashboards e relatÃ³rios analÃ­ticos.

<br>

<h4>Grupo: ThaÃ­s Bustamante, Emilly Fernandes e JoÃ£o Gabriel Rodriguez</h4>
